# Human Factors {#humans}

__Abstract:__

Even as automated tools for pattern evidence comparisons become increasingly advanced, final decisions often rely on individual examiner judgment. Given the same comparison task, different forensic examiners may come to different conclusions and/or use different criteria for coming to those decisions. High-profile examples of misidentification have inspired efforts to better estimate overall error rates as well as understand underlying decision-making processes in forensic analyses. 

Item response theory (IRT), a class of models used in educational testing (among other fields), can provide valuable insight into these studies by accounting for both individual differences among examiners as well as comparison tasks of varying difficulty. This chapter provides an introduction to basic IRT models, discussion of human factors in pattern evidence analysis, and an example analysis in R of an error rate study using the FBI ``black box'' study.

## Introduction

Although forensic measurement and analysis tools are increasingly accurate and objective, final decisions in forensic science are largely left to individual examiners, investigators, or witnesses [@pcast]. Human decision-makers continue to play a central role in forensic science, and it is unrealistic to assume that, within our current criminal justice system, either a) human decision-making can be removed from the process entirely, b) forensic decision-making tasks can be standardized to a single difficulty or discrimination, or c) these differences among individuals and/or tasks never impact the outcome of a case. 

There are (at minimum) two ways in which individual differences among forensic decision-makers manifest. The first is in the decision-makers' underlying proficiency. Ignoring differences in underlying proficiency among a group, which often include trainees or non-experts, may bias the overall results from a proficiency test or laboratory study. There may also exist differences in the decision-making thresholds used by individuals. Many forensic decisions can be cast as an evaluation of the source of evidence (e.g. Did fingerprint A come from person X?). Currently, source conclusions are often reported as an identification (same source), exclusion (different sources), or inconclusive. Different decision-makers may use different thresholds for determining the boundary between each of these three options, and accounting for these threshold differences may lead to a better understanding of examiner performance and provide strategies for examiner training.

In addition to differences among the decision-makers, there also exist differences among the tasks themselves. Properties of the evidence, such as the quality, quantity, concentration, or rarity of properties may make it easier or harder to evaluate. Some pieces of evidence, regardless of how skilled the examiner is, will not have enough information to result in an identification or exclusion in a comparison task. An inconclusive response, in this case, should be treated differently than an inconclusive response for a more straightforward comparison. In addition to the physical evidence itself, oftentimes evidence must be pre-processed into a form conducive to examination. Physical evidence then becomes pattern evidence in the form of an image (i.e. fingerprints, firearm cartridges, handwriting, tire treads, etc.). Information in the form of specific, identifiable features is present in these images, and there may also be additional information to be found in the images themselves. These extracted features and information could provide covariates to account for the varying difficulty of comparison tasks. 

It is difficult, however, to accurately measure the effects of individual differences in forensic applications. The vast majority of forensic decision-making occurs in casework, which is not made available to researchers due to privacy concerns or active investigation policies. Besides real-world casework, data on forensic decision-making is collected through proficiency test results and in laboratory experiments. 

## Data 

TODO:

- Description of proficiency tests
- Description of error rate studies
- Relate to ed. testing scenario

We have identified two settings where data regarding human factors in forensic science is available. The first is through *proficiency tests*: annual competency exams that must be completed for forensic laboratories to maintain their certification. The second setting is *error rate studies*: independent research studies designed to measure casework error rates. As their names suggest, these two data collection scenarios serve completely different purposes. Proficiency tests are (currently) designed to assess basic competency, and mistakes are rare. Error rate studies are designed with the goal of mimicking the difficulty of evidence in casework, and mistakes are more common. Proficiency tests are often administered at the laboratory level, rather than given to individual examiners. If a laboratory has multiple latent print examiners, for example, there is no way to determine which individual analyst completed which exam, or track performance of individual examiners over time. Error rate studies, on the other hand, operate independently of laboratories and individuals decide whether to participate in the study or not. 

Stringent privacy protections exist for both proficiency test data and results from error rate studies. *More description* 

*Koehler's type 2 proficiency test* 

Results from both proficiency tests and from error rate studies can be represented as a set of individuals responding to different items, in which responses can be scored as correct or incorrect. This is not unlike an educational testing scenario such as the SAT, ACT, GRE, etc, where students (individuals) answer questions (items) either correctly or incorrectly.  There is a rich body of statistical methods for estimating student proficiency and item difficulty from test responses. *CITE IRT THINGS* 

## A Brief Overview of Item Response Models 

More formally, for $P$ individuals responding to $I$ items, we can express the binary responses as a $P\times I$ matrix, $Y$. Item Response Theory (IRT) is based on the idea that the probability of a correct response depends on individual *proficiency*, $\theta_p, p = 1, \ldots, P$, and item *difficulty*, $b_i, i = 1, \ldots I$. 

A strength of item response models is their capacity to estimate person proficiencies and item difficulties separately, but on the same scale, allowing for a direct comparison. 

### Rasch Model 

The Rasch Model [@rasch1960studies @raschbook] is a relatively simple, yet powerful, item response model, and serves as the base for extensions introduced later. The probability of a correct response is modeled as the logit function of difference of the person proficiency, $\theta_p$ ($p=1, \dots, P$), and the item difficulty, $b_i$ ($i=1, \dots, I$):

\begin{equation}
\text{logit}(P(Y_{pi} = 1)) = \theta_p - b_i. 
\label{rasch}
\end{equation}

To identify the model, we use the convention of constraining the mean of the person parameters ($\mu_\theta$) to be equal to zero. This allows for a nice interpretation of both person and item parameters relative to the "average person". If $\theta_p >0$, person $p$ is of "above average" proficiency and if $\theta_p <0$, person $p$ is of "below average" proficiency. Similarily, if $b_i < 0$ question $i$ is an "easier" question and the average person is more likely to correctly answer question $i$. If $b_i >0$ then question $i$ is a "difficult" question and the average person is more likely to incorrectly answer question $i$. Other common conventions for identifying the model include setting a particular $b_i$ or the mean of the $b_i$s equal to zero. 

The item characteristic curve (ICC) describes the relationship between proficiency and performance on a particular item (see Figure \@ref(fig:extensionsexample) for examples). For item parameters estimated under a Rasch model, all ICCs are standard logistic curves with different locations on the latent difficulty/proficiency scale. 

Note that Equation~\ref{rasch} also describes a generalized linear model (GLM), where $\theta_p - b_i$ is the linear component, with a logit link function. By formulating the Rasch Model as a hierarchical GLM with prior distributions on both $\theta_p$ and $b_i$, the identifiability problem is solved. We assign $\theta_p \sim N(0, \sigma_\theta^2)$ and $b_i \sim N(\mu_b, \sigma_b^2) $, although more complicated prior distributions are certainly possible. 

### 2PL and 3PL Models 

The *two-parameter logistic model* (2PL) and *three-parameter logistic model* (3PL) are additional popular item response models [@lord1980applications]. They are both similar to the Rasch model in that the probability of a correct response depends on person proficiency and item difficulty, but additional item parameters are also included.

The 2PL model includes an item *discrimination* parameter, $a_i$, which determines the slope of the ICC:

\begin{equation}
P(Y_{pi} = 1)  = \frac{1}{1+\exp(-a_i (\theta_p - b_i))}. 
\label{2pl}
\end{equation}

Larger values of $a_i$ denote a higher discrimination, and better separation of lower/higher proficiency test-takers. This is illustrated in Figure \@ref(fig:extensionsexample) with the Rasch model in black and the 2PL models in blue, all with difficulty $b=0$ for simplicity. 

In general, items with a larger discrimination are preferred. A test including items with higher discriminations leads to increased Fisher information for $\theta$ as compared to a test with less discriminating items. An ideal test for accurately estimating theta would have a variety of highly discriminating items spread over a large range of difficulties [@van2013handbook].  

The 3PL model includes an item *pseudo-guessing* parameter, $c_i$, which determines the lower asymptote of the ICC: 

\begin{equation}
P(Y_{pi} = 1)  = c_i + \frac{1-c_i}{1+\exp(-a_i (\theta_p - b_i))}. 
\label{3pl}
\end{equation}

Including $c_i$ increases the ``base probability'' of correctly answering a question. For instance, in a classic multiple choice question, even respondents with extremely low proficiencies will sometimes get a question right by chance. 

There may be options in a multiple choice question that are obviously incorrect, to the point where even low-proficiency respondents will not choose that option, which may not be known before the exam is administered. Using an IRT approach accounts for these effects by using the observed data to estimate $c_i$, rather than determining the value of $c_i$ from the question itself. 

3PL models are illustrated in Figure \@ref(fig:extensionsexample) with green lines. These models have the same difficulty ($b=0$) and discriminations ($a=0.5$ and $a=2$) as the other models shown in the graph, but include a guessing parameter of $c=0.3$. 

```{r extensionsexample, echo = FALSE, fig.cap = "Item Characteristic Curve (ICC) examples for the Rasch, 2PL, and 3PL models."}
library(RColorBrewer)
theta = seq(-8, 8, by = .01)

brewer.colors = c(1, brewer.pal(4, "Paired"))
line.rasch = 1/(1+exp(-(theta - 0)))
line.2pl.sm = 1/(1+exp(-.5*(theta - 0)))
line.2pl.big = 1/(1+exp(-2*(theta - 0)))
line.3pl.sm = .3 + (1-.3)/(1+exp(-.5*(theta - 0)))
line.3pl.big = .3 + (1-.3)/(1+exp(-2*(theta - 0)))

plot(line.rasch~theta, type = 'l', lwd = 2,col = brewer.colors[1], ylab = "Probability", 
     xlab = expression(theta), xlim = c(-6,6), ylim = c(0,1), main = bquote(P(Y["pi"]==1)), family = 'serif')
points(line.2pl.sm~theta, type = 'l', lwd = 2,col = brewer.colors[2])
points(line.2pl.big~theta, type = 'l', lwd = 2,col = brewer.colors[3])
points(line.3pl.sm~theta, type = 'l', lwd = 2,col = brewer.colors[4])
points(line.3pl.big~theta, type = 'l', lwd = 2,col = brewer.colors[5])
legend("bottomright", legend = 
         c('Rasch, b=0', '2PL, a=0.5', '2PL, a=2', '3PL, a=0.5, c=0.3', '3PL, a=2, c=0.3'),  lty = rep(1,5),
       bty = 'n', lwd = rep(1.5,5), col = brewer.colors, cex = .8)
```

### Partial Credit Model 

The *partial credit model* (PCM, @masters1982) is distinct from the Rasch, 2PL, and 3PL models discussed above because it allows for the response variable, $Y_{pi}$, to take additional values beyond zero (incorrect) and one (correct). This is especially useful for modeling partially correct responses, although may be applied in other contexts where the responses can be ordered. When $Y_{pi}$ is binary, the partial credit model is equivalent to the Rasch model. Under the PCM, the probability of response $Y_{pi}$ depends on $\theta_p$, the proficiency of person $p$ as in the above models; $m_i$, the maximum score for item $i$ (and the number of step parameters); and $\beta_{il}$, the $l^{th}$ step parameter for item $i$ ($l=0, \dots, m_i$):

\begin{equation}
P(Y_{pi} = 0)  = \frac{1}{1+\sum_{k=1}^{m_i} \exp \sum_{l=1}^k (\theta_p - \beta_{il})} \\
P(Y_{pi} = y, y>0)  = \frac{\exp\sum_{l=1}^y(\theta_p - \beta_{il})}{1+\sum_{k=1}^{m_i}\exp \sum_{l=1}^k (\theta_p - \beta_{il})}.
\label{pcm}
\end{equation}

We display an example PCM below by plotting the probabilities of observing each of three categories as a function of $\theta_p$ (analagous to the ICC curves above). 

```{r pcmexample, echo = FALSE, fig.cap = "Category response functions for the PCM."}
brewer.colors = brewer.pal(4, "Set1")[2:4]
theta = seq(-8, 8, by = .01)
beta1=-2
beta2=2
line.cat0 = 1/(1+exp(theta-beta1) + exp(2*theta - beta1 - beta2))
line.cat1 = exp(theta-beta1)/(1+exp(theta-beta1) + exp(2*theta - beta1 - beta2))
line.cat2 = exp(2*theta-beta1-beta2)/(1+exp(theta-beta1) + exp(2*theta - beta1 - beta2))

plot(line.cat0~theta, type = 'l', lwd = 2,col = brewer.colors[1], ylab = "Probability", 
     xlab = expression(theta), xlim = c(-6,6), ylim = c(0,1), main = bquote(P(Y["pi"]==0,1,2)), family = 'serif')
points(line.cat1~theta, type = 'l', lwd = 2,col = brewer.colors[2])
points(line.cat2~theta, type = 'l', lwd = 2,col = brewer.colors[3])
abline(v=c(beta1, beta2), lty = 2)
text(beta1+.6, .9, bquote(beta[1] == .(beta1)))
text(beta2+.6, .9, bquote(beta[2] == .(beta2)))
legend("right", legend = 
         c(bquote(Y[pi]==0), bquote(Y[pi]==0), bquote(Y[pi]==0)),  lty = rep(1,3),
       bty = 'n', lwd = rep(1.5,3), col = brewer.colors, cex = .8)
```

## R Package(s)

- `Stan`: estimate parameters for the model using MCMC 
- `rstan`: implementation of Stan for R 
- `blackboxstudyR`: provides some standard IRT models in Stan and functions for working with the black box data

## Drawing Conclusions

Results from an IRT analysis can be separated into two categories: 
1. Information about the test itself (e.g. question difficulty)
2. Information about the test-takers (e.g. proficiency)


## Case Study

The FBI ``black box'' study [@ulery2011] was the first large-scale study to assess the accuracy and reliability of latent print examiners.  169 participants examined 50-100 pairs of latent and reference fingerprint from a larger pool of 744 pairs. The questions were designed to include a range of attributes and quality seen in casework, and to be representative of searches from an automated fingerprint identification system. Although the purpose of the study was to estimate false positive and false negative error rates, examiners were also given an "inconclusive" option to evaluate pairs of prints.

Inconclusive responses complicate the scoring of the exam due to both their ambiguity and prevalence. `Inconclusive' is never keyed as the correct response by the FBI, which leads to the ambiguity in scoring. There are a large number of inconclusive answers (4,907 of 17,121 responses), and examiners vary on which latent print pairs are inconclusive. 

TODO: distributions of inconclusive responses for each examiner and item

To explore the impact of inconclusives on proficiency exams, the inconclusive responses were scored in five ways:

1. Score all inconclusive responses as incorrect (*Inconclusive Incorrect*). This may penalize participants who were shown more vague or harder questions and therefore reported more inconclusives. 
2. Treat inconclusive responses as missing completely at random (*Inconclusive MCAR*). This decreases the amount of data included in the analysis, and does not explicitly penalize examiners who report many inconclusives. This is the scoring method most similar to the method used in [@ulery2011] to compute false positive and false negative rates. 
3. Score inconclusive as correct if the reason given for an inconclusive is "correct". Since the ground truth "correct" inconclusive reason is unknown, the consensus reason from other inconclusive responses for that question is used. If no consensus reason exists, the inconclusive response was scored in one of two ways:
	a. Treat inconclusive responses as incorrect if no consensus reason exists (*No consensus incorrect*). 
	b. Treat inconclusive responses as missing completely at random if no consensus reason exists (*No consensus MCAR*). 
4. Score inconclusive responses as ``partial credit'' (*Partial Credit*). 

TODO: Comparisons of IRT results for each of the scoring schemes

TODO: Discussion of results
